---
title: "Heart Disease Prediction Using a Random Forrest Model in R"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

When I first began learning about data science, machine learning seemed unapproachable. I thought that to understand machine learning algorithims I would to master statistics and linear aglebra and to implemnt them I would need to be a brilliant programmer. As my math skills improved, I began to grow more comfortable with model fitting and while I could easiy build a linear regression model in R, I had no idea where to begin as far as implementing more advanced models.

That all changed after I attended this years (virtual) Open Data Science conference and participated in an R machine learning training using TidyModels. I saw how these ingenious packages make it easy to fit, test, tune and evaluate a wide variety of diverse model types without getting bogged down in disparate syntax.

After attending some hands on tutorials using various types of decision tree models, I decided to try my hand at using a random forest classification model. The University of California at Irvine curates excellent real world data sets, and I decided to use their heart disease dataset to predict if individuals had heart disease. Here is the url: https://archive.ics.uci.edu/ml/datasets/Heart+Disease.

You will see that the UCI has data from four different hospitals however a combined version was uploaded to kaggle. In this version the target (outcome) variable was converted into 0 or 1 (pressence of heart disease or not) instead of 0 - 4. Here is the url to this version of the data which I used: https://www.kaggle.com/ronitf/heart-disease-uci.

Random forest models are based on decision tree models, a simple model type of model where each data point is evaluated on whether or not it passes a logical test based on one or more variables. The result of the test determines which branch of the tree that data point will continue down, determining the next logical test it will encounter. After passing through enough nodes to catagorize the data point with enough specificity to assign it a discrete classification its placed in a 'bin'. In a classification model, each bin represents one of the possible outcome catagories. A random forest is a bootstrapped version of a decision tree, where a large number of decision trees are calculated and the mode result is taken, often producing a more accurate model. 

Let's start by loading the packages we need and reading the data set.
```{r, message=FALSE, collapse=TRUE}
##loading important packages
library(tidymodels)
library(tidyverse)
library(workflows)
```

```{r}
##reading the data into R
dat <- read.csv('heart.csv')
```

```{r, echo =FALSE, results='hide'}
dat[,14]<- as.factor(dat[,14])
```

```{r}
##taking a look at the data
glimpse(dat)
summary(dat)
```

We can see that that there are thirteen predictors and one outcome and that some variables like thalach are continuous while others like sex are discreet (1 = male 0 = female)

We can also see that age has a mean of 54.37 and a min value of 29 which suggests that our data skews to older adults. While this is this is not surprising considering that our data comes from individuals tested for heart disease risk factors, it does tell us that we should be hesitant to apply our model in predicting outcomes for datasets of younger adults. 

Now let's see how some of these variables are distributed
```{r}
table(dat$target)
```
There are more individuals with heart disease in the data, but not by a significant amount.

```{r, message=FALSE}
##creating histograms to view variable distribution
for (i in 1:(ncol(dat)-1)) {
  print(ggplot(dat) + 
          geom_histogram(aes(dat[,i]))+
          xlab(colnames(dat)[i]))
}
```

Variables like age, resting blood preasure(tresbps), serum cholestoral (chol), and max heart rate (thalach) seem to be more uniformly distributed, while ST depression induced by exercise(oldpeak), number of major vessels colored by flourosopy (ca) and thal are more skewed. We can also see that our data is not gender balanced as there are around twice as many males as females. 

```{r, message=FALSE}
##creating violin plots to see differences in distribution for the target outcomes
for (i in 1:(ncol(dat)-1)) {
  print(ggplot(dat) + 
          geom_violin(aes(dat[,i], target))+
          xlab(colnames(dat)[i]))
}
```

These fancy violin plots shed some light on the distribution of the predictor variables in the two outcome groups. Variables like number of major vessels colored by flourosopy (ca), chest pain type (cp), max heart rate (thalach), thal and slope of the peak exercise ST segment (slope) seem to vary significantly and will likely be important in our model. Resting heart rate, fbs, restecg, and sex seem to matter less.

Now lets get ready to build our model. First we need to split the data into training and test sets.

```{r}
##splitting the data up into training and test sets
set.seed(123)
split_dat <- initial_split(dat)
train_dat <- training(split_dat)
test_dat <- testing(split_dat)

split_dat
```
We'll train our data on the training set of 228 and then test and evaluate on the test set of 75. 

In order to optimize our model using the tune package, we need to create a sample of the data to test different parameter values on. In this example I use vfold_cv() to create a dataframe composed of 10 randomly selected groups of data points (known as folds) from our training set. We then test each model with different parameter values on each of the 10 folds. Then we can evaluate each model's performance and select the optimal parameters. Testing different parameter values allows for a more accurate model and conducting these test on 10 different folds helps to prevent overfiting as performance is evaluated on 10 different samples. This process is known as cross validation.

```{r}
##creating a cross validated version of the training set to tune parameters
dat_cv <- vfold_cv(train_dat)
```

Now its time to start preprocessing our model! We'll use the recipes package to specify our outcome variable target, and our predictor variables (all 13 others). Since there are no missing values, we don't need to add an imputation step, but we could easily with something like step_impute_linear() or step_knnimpute(). Since many of our variables have vastly different ranges, it's a good idea to normalize our data which I do hear with step_normalize().
```{r}
rand_for_recipe <- 
  recipe(target ~ ., data = dat) %>%
  step_normalize(all_numeric())
```

Now we'll specify the type of model, that we will tune the mtry paremeter, that we'll use the ranger engine, our method of variable importance, and that our mode is classification.
```{r}
##specifying the use of a random forest model 
rand_for_model <- rand_forest() %>%
##specifying that we will be tuning mtry
##(number of variables that can be split on at each tree node)
  set_args(mtry = tune()) %>%
##specifying the engine and the mode of variable importance
  set_engine("ranger", importance = "impurity") %>%
##selecting the mode, for ranger the options are "classification" and "regression"
  set_mode("classification")

```

Next we combine the recipe and model into a workflow for easy tuning and fitting.

```{r}
##adding the the recipe and model to a workflow
rand_for_workflow <- 
  workflow() %>% 
  add_recipe(rand_for_recipe) %>% 
  add_model(rand_for_model)
```

Now that our workflow is set up we can tune the mtry parameter. We'll use expand_grid to create a dataframe of possible values, select relevant metrics - in this case auc and accuracy, and then calculate.
```{r}
##creating a dataframe of mtry values to test
rand_for_tune_grid <- expand_grid(mtry = c(2,3,4,5))
##calculating the tune results
rand_for_tune_results <- rand_for_workflow %>%
  tune_grid(resamples = dat_cv, 
            grid = rand_for_tune_grid, 
            ##selecting relevent metrics to assess fit
            metrics = metric_set(roc_auc, accuracy))
```

Let's take a look at the results:
```{r}
collect_metrics(rand_for_tune_results)
```

Now we'll select the value with the best AUC and add it to our workflow.
```{r}
##using select_best() to choose the mtry value with the best auc
rand_for_final <- 
  rand_for_tune_results %>% 
  select_best(metric = "roc_auc")

##adding the final(tuned) parameter to the workflow to finalize
rand_for_workflow <- 
  rand_for_workflow %>% 
  finalize_workflow(rand_for_final)
```

Now we are ready to fit our model *finally*! The handy last_fit() function allows us to fit the model on the training set, evaluate it on the test set and returns a tibble of predicted values and evaluation metrics. 
```{r, collapse=FALSE}
rand_for_fit <- 
  last_fit(rand_for_workflow, split_dat)

rand_for_fit
```

Let's see how our model performed.
```{r}
##collecting metrics to evaluate the model
collect_metrics(rand_for_fit)
```

Those performance metrics look excellent! Let's look at a breakdown of predicted values with actual values.
```{r, collapse=TRUE}
##collecting the model's prediction for each patient
model_predictions <- 
  collect_predictions(rand_for_fit)

##returning a confusion matrix of the model's predictions and actual results
model_predictions %>%
  conf_mat(target, .pred_class)
```

Now lets try to figure out how we got there. First we'll train our model on the entire data, and save that model object which we can use on future data.
```{r}
final_model <- 
  fit(rand_for_workflow, dat)
```
Now we can extract the fit object from the data and examine the variable importance.
```{r}
##extracting the fit object
model_obj <- 
  pull_workflow_fit(final_model)$fit

model_obj

##examining variable importance
model_obj$variable.importance
```
